{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Word Embedding은 구조적으로 dictionary(bag of words) 방식의 limited Vector Space에 한계가 있다. 즉, 알려진 단어 위주의 학습. \n",
    "본 Jupyter Notebook은 Log file 등의 layout 중심의 Document classification을 위해서는 Character-Level의 분류기를 설계할 필요가 있다 판단하여 관련 지식 및 논문을 정리해본다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icell/anaconda3/envs/hog/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고 논문\n",
    "*'Character-level Convolutional Networks for Text Classification', Xiang Zhang et al*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 논문 中 2.2 Character quantization\n",
    "...Our models accept a sequence of encoded characters as input. The encoding is done by prescribing an alphabet of size m for the input language, and then quantize each character using 1-of-m encoding (or \"one-hot\" encoding). Then, the sequence of characters is transformed to a sequence of such m sized vectors with fixed length L0. Any character exceeding length L0 is ignored, and <span style=\"color:blue\">any characters that are not in the alphabet including blank characters are quantized as all-zero vectors. </span> \n",
    "\n",
    "- 공백(Blank)은 0이 아닌 고유의 Vector 값을 추가 부여\n",
    "- L0 size를 분류대상 문서의 일부 threshold로 설정 (100 lines but <span style=\"color:red\">how can we count the characters in one line)</span>\n",
    "\n",
    "...The character quantization order is backward so that the latest reading on charaters is always placed near the begin of the output, making it easy for fully connected layers to associate weights with the latest reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alphabet used in all of our models consists of 70 characters, <span style=\"color:green\">==> **71** characters</span>\n",
    "\n",
    "- including 26 english letters : **abcdefghijklmnopqrstuvwxyz**\n",
    "- 10 digits : **0123456789**\n",
    "- 33 other characters and the new line character : **<span>-,;.!?:’’’/\\|_@#$%ˆ&*˜‘+-=<>()[]{}</span>**\n",
    "- <span style=\"color:green\">(Customize) add character : **공백(Black)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add blank at the end of string variable\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{} \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Dataset은 산문(Report), 규격화된 표(Table) data에 Label 정보명을 File 명으로 하는 File Dataset이다.\n",
    "\n",
    "예를 들어, \n",
    "\n",
    "- Filename : report01.txt\n",
    "- Contents : \n",
    "            ....Studying examples of poems using various poetic devices helps such as context create an understanding of how those poetry terms work within different types of poetry.  For instance examples of poems using onomatopoeia can illustrate how sounds can be represented in poems.  Likewise, examples of poems using alliteration can shed light on how alliteration affects the rhythm of a poem.  Many poems can be an example of context, but sometimes good examples are hard to find.  You'll find relevant, concise poetry examples here. \n",
    "            \n",
    "- Filename : table01.txt\n",
    "- Contents :\n",
    "\n",
    "                 ==============================================================================\n",
    "                  ID              NAME                AGE                  PHONE\n",
    "                 ==============================================================================\n",
    "                  machine01       John                28                   089-7835-1945\n",
    "                  k-meaning       Anderson            40                   1-333-5321-3542\n",
    "                 ....\n",
    "                 \n",
    "두 데이터의 Bag of Words 차이는 거의 없다고 할 때, 문서 구조적인 차이만 보이고 있다. Character Level의 분류가 잘 작동하는지가 본 Notebook의 실험 주요 목표이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 논문 中 2.3 Model Design\n",
    "...We designed 2 ConvNets - one large and one samll. They are both 9 layers deep with 6 convolutional layers and 3 fully-connected layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/model.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input have number of features equal to 70 due to our character quantization method, and the input feature length is 1014. It seems that 1014 characters could already capture most of the texts of interest. We also insert 2 dropout [10] modules in between the 3 fully-connected layers to regularize. They have dropout probability of 0.5. **Table 1** lists the configurations for convolutional layers, and **table 2** lists the configurations for fully-connected (linear) layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability = 0.5\n",
    "\n",
    "def do_dropout(x):\n",
    "    return tf.nn.dropout(x, probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 1 \n",
    "Convolutional layers used in our experiments. The convolutional layers have stride 1 and pooling layers are all non-overlapping ones, so we omit the description of their strides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/table1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_1(x_input):\n",
    "    \n",
    "    # 71 X 1014 size of x_input\n",
    "\n",
    "    #Layer 1\n",
    "    conv1 = tf.layers.conv1d(inputs=x_input, filters=256, strides=1, kernel_size=7, activation=tf.nn.relu)\n",
    "    conv1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=3, strides=1)\n",
    "\n",
    "    #Layer 2\n",
    "    conv2 = tf.layers.conv1d(inputs=conv1, filters=256, strides=1, kernel_size=7, activation=tf.nn.relu)\n",
    "    conv2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=3, strides=1)\n",
    "    \n",
    "    #Layer 3\n",
    "    conv3 = tf.layers.conv1d(inputs=conv2, filters=256, strides=1, kernel_size=3, activation=tf.nn.relu)\n",
    "    \n",
    "    #Layer 4\n",
    "    conv4 = tf.layers.conv1d(inputs=conv3, filters=256, strides=1, kernel_size=3, activation=tf.nn.relu)\n",
    "    \n",
    "    #Layer 5\n",
    "    conv5 = tf.layers.conv1d(inputs=conv4, filters=256, strides=1, kernel_size=3, activation=tf.nn.relu)\n",
    "    \n",
    "    #Layer 6\n",
    "    conv6 = tf.layers.conv1d(inputs=conv5, filters=256, strides=1, kernel_size=3, activation=tf.nn.relu)\n",
    "    conv6 = tf.layers.max_pooling1d(inputs=conv6, pool_size=3, strides=1)\n",
    "\n",
    "    return conv6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/table2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_2(x_input):\n",
    "    \n",
    "    x = tf.reshape(x_input, [-1, 44*256])\n",
    "    \n",
    "    fc1 = tf.layers.flatten(x_input)\n",
    "    fc1 = tf.layers.dense(fc1, 1024)\n",
    "    fc1 = do_dropout(fc1)\n",
    "    \n",
    "    fc2 = tf.layers.flatten(fc1)\n",
    "    fc2 = tf.layers.dense(fc2, 1024)\n",
    "    fc2 = do_dropout(fc2)\n",
    "    \n",
    "    fc3 = tf.layers.flatten(fc2)\n",
    "    fc3 = tf.layers.dense(fc3, 10)\n",
    "    output = do_dropout(fc3)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(np.float32, [None, 55000, 784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table1 = table_1(X)\n",
    "Table2 = table_2(Table1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_21/mul:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./mnist/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = mnist.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000,)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = mnist.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(Table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.train.AdamOptimizer(learning_rate=0.002).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmptvdy98yr\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_evaluation_master': '', '_save_summary_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_task_type': 'worker', '_save_checkpoints_secs': 600, '_global_id_in_cluster': 0, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_train_distribute': None, '_log_step_count_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd2800087b8>, '_service': None, '_master': '', '_model_dir': '/tmp/tmptvdy98yr', '_keep_checkpoint_max': 5, '_task_id': 0, '_tf_random_seed': None, '_is_chief': True}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmptvdy98yr/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3138804, step = 0\n",
      "INFO:tensorflow:global_step/sec: 363.002\n",
      "INFO:tensorflow:loss = 0.11311814, step = 100 (0.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.561\n",
      "INFO:tensorflow:loss = 0.06954762, step = 200 (0.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 416.632\n",
      "INFO:tensorflow:loss = 0.118468136, step = 300 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 438.153\n",
      "INFO:tensorflow:loss = 0.0055782706, step = 400 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 440.203\n",
      "INFO:tensorflow:loss = 0.05372288, step = 500 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.429\n",
      "INFO:tensorflow:loss = 0.057074156, step = 600 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 450.196\n",
      "INFO:tensorflow:loss = 0.032950163, step = 700 (0.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 460.308\n",
      "INFO:tensorflow:loss = 0.03931855, step = 800 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 466.811\n",
      "INFO:tensorflow:loss = 0.10490177, step = 900 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 417.328\n",
      "INFO:tensorflow:loss = 0.03311105, step = 1000 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 409.413\n",
      "INFO:tensorflow:loss = 0.013406029, step = 1100 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 432.383\n",
      "INFO:tensorflow:loss = 0.022338053, step = 1200 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 434.951\n",
      "INFO:tensorflow:loss = 0.01288021, step = 1300 (0.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.409\n",
      "INFO:tensorflow:loss = 0.016624384, step = 1400 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.521\n",
      "INFO:tensorflow:loss = 0.042729773, step = 1500 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 451.865\n",
      "INFO:tensorflow:loss = 0.0025068568, step = 1600 (0.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 447.919\n",
      "INFO:tensorflow:loss = 0.033585403, step = 1700 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.586\n",
      "INFO:tensorflow:loss = 0.06707162, step = 1800 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 470.526\n",
      "INFO:tensorflow:loss = 0.005948185, step = 1900 (0.213 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/tmptvdy98yr/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.021487573.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-08-15:14:48\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmptvdy98yr/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-08-15:14:49\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.9894, global_step = 2000, loss = 0.037395727\n",
      "Testing Accuracy: 0.9894\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 2000\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.25 # Dropout, probability to drop a unit\n",
    "\n",
    "\n",
    "# Create the neural network\n",
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict['images']\n",
    "\n",
    "        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        # Convolution Layer with 64 filters and a kernel size of 3\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        # Fully connected layer (in tf contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode):\n",
    "    # Build the neural network\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits_train = conv_net(features, num_classes, dropout, reuse=False,\n",
    "                            is_training=True)\n",
    "    logits_test = conv_net(features, num_classes, dropout, reuse=True,\n",
    "                           is_training=False)\n",
    "\n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits_test)\n",
    "\n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "\n",
    "        # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op,\n",
    "                                  global_step=tf.train.get_global_step())\n",
    "\n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "\n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=pred_classes,\n",
    "        loss=loss_op,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs\n",
    "\n",
    "# Build the Estimator\n",
    "model = tf.estimator.Estimator(model_fn)\n",
    "\n",
    "# Define the input function for training\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.train.images}, y=mnist.train.labels,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "# Train the Model\n",
    "model.train(input_fn, steps=num_steps)\n",
    "\n",
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.test.images}, y=mnist.test.labels,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "e = model.evaluate(input_fn)\n",
    "\n",
    "print(\"Testing Accuracy:\", e['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
